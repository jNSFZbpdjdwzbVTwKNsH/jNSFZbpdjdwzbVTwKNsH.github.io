---
title: MoE and Latent Attention
layout: page
author: Carlo Rosso
date: 2025-11-20
---

Inspired by MoE which is a technology to boost performance of the transformer.

Basically in the decoder block of the transformer theere is a feed forward
network that takes in input the output of the self-attention, computes a linear
combination of it in a higher dimensional space and project it back to the same
input dimension.

MoE uses a router mechanism to multiple lower higher dimensional spaces.
The idea is to use some kind of attention to select the higher dimensional
space.

In such a way there isn't exactly an expert, but every sample builds its own
expert.
To implement this in practice you can think about it in the Latent Attention
fashion. Basically the full feed-forward layer would be of this kind:

$
ouput = x^T W
$

Where I suggest to learn:

$
output = x^T \hat{P} \hat{W}
$

where $W \in \mathbb{R}^{d \times d}$, $\hat{P} \in \mathbb{R}^{d \times m}$, 
and $\hat{W} \in \mathbb{R}^{m \times d}$. Notably for $m << d$ the computation
gets much more efficient.
Furthermore, if possible I'm curious to investigate:
$\hat{P} \in \{0, 1\}^{m \times m}$, and $\hat{W} \in \mathbb{R}^{d \times d}$.
