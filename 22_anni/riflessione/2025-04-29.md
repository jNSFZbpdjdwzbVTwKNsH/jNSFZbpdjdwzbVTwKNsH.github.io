--- 
layout: page-private
title: 29/04
category: riflessione
---

Build an algorithm that starts with few neurons/parameters.
It compute the VC dimension on itself. To check whether it needs more neurons.
The concept is:
1. you add clean neurons for $n$ samples
2. you teach $n$ new samples to the model (preventing the model to forget old samples)
3. you take off the unused weights
4. repeat

Also, is there a way to compute the range parameters can change so that
their answer doesn't change. I imagine the problem as a linear problem. Is there
a way to identify the polygon that respects the constraints imposed by the
samples? If so, the problem gets to be: when the system does not admit a
solution you need to improved the parameters number. Over the VC dimension we
are interested in the moment we need to add some neuron: the samples are to be
overlapping. At least many of them, so the VC dimension is very much an upper
bound.
