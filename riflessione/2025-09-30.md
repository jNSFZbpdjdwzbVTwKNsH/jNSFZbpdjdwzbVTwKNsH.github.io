---
title: 30/09
layout: page
author: Carlo Rosso
date: 2025-09-30
---

Transformer are good at long range interaction, but they have quadratic 
complexity, and thus they are not usable in context where the input is too big.
Therefore, I came up with an idea to reduce the number of connections in the 
neural network.
1. You pick a distribution $\mathcal(D)$, e.g. normal distribution, student distribution, ...
1. Each head is defined by $\text{attention_mask} \sim [\mathcal(D)]^k$
    - Where you extract $k \sim \mathcal(E)$
    - $\text{attention_mask}_i \subseteq [-n, n]$
    - Each head has in input only the values that are in its attention mask
    - And so every head has $k$ weights
1. Finally, you train the neural network as a normal neural network

The first research could go as follow:
1. Repeat for $k in [n, n-1, ...]$ as long as $\mathbb{E}[\ell_a] \approx \mathbb{E}[\ell_n]$:

    2. Let $\mathcal{G}$ be a distribution i.e. normal distribution
    3. Let $\mathcal{C}$ be a distribution i.e. gamma distribution
    3. For each attention head $i$:

        4. Sample the number of connections to consider $c_i \in \mathcal{G}$, such that $\mathbb{E}[c_i] = k$
        5. Sample which connection to consider relatively to a fixed 
           feature. i.e. $C_i \subset [1, n]$ such that $C_{i}_j \sim \mathcal{C}$.
        6. Each attention mask will consider $\{C_{i}_j, -C_{i}_j: C_{i}_j in C_i\}$.

---

Another research can 
1. Rank the heads
2. Implement an algorithm to change them in training
3. Figure out a parameterized distribution to sample better heads

For example, the algorithm can
- Take off very similar heads
- Take off bad ranking heads
- Sample from the same initial distribution
- Sample from the same distribution of the best ranking heads
- Sample from a distribution parameterized on the remaining distributions
- Be a reinforced neural network, where the reward is based on the heads ranking 
- Be a neural network, whose input are the remaining heads

Effectively a neural network approximate a probability distribution.
The reinforced neural network should get reward also based on the computational 
complexity of the neural network. Not on the length of the training, but it
should prefer less connections over total connections.

The heads ranking can be based on:
- Some entropy
- Heads independence
- Outgoing weights, with some normalization

As this is a network that changes its architecture in training, it should take
many more experiments and cannot be an initial research. Also you can see there
are many different variants, and so I still need to find the path to approach
the problem.
