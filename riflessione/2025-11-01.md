---
title: 01/11
layout: page
author: Carlo Rosso
date: 2025-11-01
---

Taking the previous idea, I talked with a professor who suggested the following
approach: [Latte: Latent Attention for Linear Time Transformers](https://arxiv.org/abs/2402.17512).

I think that the approach is very interesting. To implement my idea with this
approach maybe we can sample the dimension of $p$ from a random distribution.
When a layer doesn't work correctly you sample a new value of $p$ to fix the
issue.

This makes the idea much simpler, and maybe not needed.
