---
title: CNN with dynamic kernel selection
layout: page
author: Carlo Rosso
date: 2025-09-18
---

Transformer are good at long range interaction, but they have quadratic 
complexity, and thus they are not usable in context where the input is too big.
Let's reduce the number of connections in the neural network:
1. You pick a distribution $\mathcal(D)$, e.g. normal distribution, student distribution, ...
1. Each head is defined by $\text{attention_mask} \sim [\mathcal(D)]^k$
    - Where you extract $k \sim \mathcal(E)$
    - $\text{attention_mask}_i \in [-n, n]$
    - Each head has in input only the values that are in its attention mask
    - And so every head has $k$ weights
1. Finally, you train the neural network as a normal neural network

---


The first research could go as follow:
1. Make $\mathcal{E}$ be a constant distribution
2. Pick a reasonable distribution, i.e. normal distribution
3. Change $k \in [a, n]$, in the opposite direction as long as $\mathcal{E}[\ell_a] \approx \frac{3}{4} \mathcal{E}[\ell_n]$
4. Experiment different distribution $\mathcal{D}$
5. Experiment different distribution $\mathcal{E}$

---

- Another research can check out whether different layers are more or less affected by $k$

---

Another research can 
1. Rank the heads
2. Implement an algorithm to change them in training
3. Figure out a parameterized distribution to sample better heads

For example, the algorithm can
- Take off very similar heads
- Take off bad ranking heads
- Sample from the same initial distribution
- Sample from the same distribution of the best ranking heads
- Sample from a distribution parameterized on the remaining distributions
- Be a reinforced neural network, where the reward is based on the heads ranking 
- Be a neural network, whose input are the remaining heads

Effectively a neural network approximate a probability distribution.
The reinforced neural network should get reward also based on the computational 
complexity of the neural network. Not on the length of the training, but it
should prefer less connections over total connections.

The heads ranking can be based on:
- Some entropy
- Heads independence
- Outgoing weights, with some normalization

As this is a network that changes its architecture in training, it should take
many more experiments and cannot be an initial research. Also you can see there
are many different variants, and so I still need to find the path to approach
the problem.
