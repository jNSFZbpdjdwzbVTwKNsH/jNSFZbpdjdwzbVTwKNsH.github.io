---
title: Random Attention
layout: page
author: Carlo Rosso
date: 2025-11-27
---

I have two proposal to diminish squared complexity in attention:
1. Set the dropout to something like $p = n / n^2$ where $n$ is the length of
   the input in the fully connected layer that aims to merge all the embeddings
   in one.
   You can compute the dropout beforehand to avoid useless computation.
