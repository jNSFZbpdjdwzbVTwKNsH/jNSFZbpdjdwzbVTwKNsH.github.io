---
title: NLP/Embedding creation
layout: page
author: Carlo Rosso
date: 2025-09-11
---

Idea: to build the embeddings of words, we can concatenate the output of every
layer. We keep very tiny layers. And they grow bigger in depth.

For example:

1. `x` -> input to model
2. `y_1 = w_1 @ x` -> output first layer
3. `y_2 = np.concat(w_2 @ y_1, y_1)` -> output second layer
4. `y_3 = np.concat(w_3 @ y_2, y_2)` -> output third layer
...

In this way the size of the embeddings doubles in size at every passage.
Notably layer 1 and layer 2 has the same size, so it's more like Fibonacci's way
of growing.

Once you get to a good size, you can keep elaborating from it.
