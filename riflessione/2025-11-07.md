---
title: 07/11
layout: page
author: Carlo Rosso
date: 2025-11-07
---

Considering the notes of my professor, where you train a transformer on
with full attention, but to make it linear you use PQ, PK, and PV, you can do
the full attention transformer with Q_1 Q_2, K_1 K_2, V_1 V_2 and then you do
rows drop out for index 1 and columns drop out for index 2, with some aggressive
drop out at the start you end up with a model identical to the first one, but
possibly it converges to a better result. Namely at the start each matrix has
the same dimension of the original one (Q, K, and V respectively).
